#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M√¥ h√¨nh d·ª± ƒëo√°n s·ªë x·ªï s·ªë s·ª≠ d·ª•ng RNN v·ªõi LSTM layers
H·ªó tr·ª£ c√°c lo·∫°i d·ª± ƒëo√°n: raw_numbers, sum, counts
"""

import numpy as np # type: ignore
import pandas as pd # type: ignore
import tensorflow as tf # type: ignore
from tensorflow import keras # type: ignore
from tensorflow.keras import layers # type: ignore
from sklearn.preprocessing import MinMaxScaler # type: ignore
from sklearn.model_selection import train_test_split # type: ignore
import matplotlib.pyplot as plt # type: ignore
import seaborn as sns # type: ignore
import warnings
import os
import glob
from datetime import datetime

warnings.filterwarnings('ignore')

class LotteryDataProcessor:
    """X·ª≠ l√Ω d·ªØ li·ªáu x·ªï s·ªë"""
    
    def __init__(self, data_file):
        self.data_file = data_file
        self.scaler = MinMaxScaler()
        
    def load_data(self):
        """ƒê·ªçc d·ªØ li·ªáu t·ª´ file"""
        print("ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ file...")
        with open(self.data_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # L·ªçc v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu
        numbers = []
        for line in lines:
            line = line.strip()
            if line.isdigit() and len(line) == 3:
                numbers.append(int(line))
        
        print(f"ƒê√£ ƒë·ªçc {len(numbers)} s·ªë x·ªï s·ªë")
        return numbers
    
    def create_sequences(self, data, sequence_length=10):
        """T·∫°o chu·ªói d·ªØ li·ªáu cho m√¥ h√¨nh RNN"""
        X, y = [], []
        
        for i in range(len(data) - sequence_length):
            X.append(data[i:i + sequence_length])
            y.append(data[i + sequence_length])
        
        return np.array(X), np.array(y)
    
    def prepare_raw_numbers_data(self, sequence_length=10):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho d·ª± ƒëo√°n s·ªë nguy√™n"""
        numbers = self.load_data()
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ kho·∫£ng [0, 1]
        numbers_array = np.array(numbers).reshape(-1, 1)
        numbers_normalized = self.scaler.fit_transform(numbers_array).flatten()
        
        # T·∫°o chu·ªói
        X, y = self.create_sequences(numbers_normalized, sequence_length)
        
        # Chuy·ªÉn ƒë·ªïi y v·ªÅ d·∫°ng one-hot encoding cho 1000 s·ªë (000-999)
        y_one_hot = tf.keras.utils.to_categorical(y * 999, num_classes=1000)
        
        return X, y_one_hot, self.scaler
    
    def prepare_sum_data(self, sequence_length=10):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho d·ª± ƒëo√°n t·ªïng c√°c ch·ªØ s·ªë"""
        numbers = self.load_data()
        
        # T√≠nh t·ªïng c√°c ch·ªØ s·ªë
        sums = []
        for num in numbers:
            digit_sum = sum(int(digit) for digit in str(num).zfill(3))
            sums.append(digit_sum)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        sums_array = np.array(sums).reshape(-1, 1)
        sums_normalized = self.scaler.fit_transform(sums_array).flatten()
        
        # T·∫°o chu·ªói
        X, y = self.create_sequences(sums_normalized, sequence_length)
        
        # Chuy·ªÉn ƒë·ªïi y v·ªÅ d·∫°ng one-hot encoding cho 28 s·ªë (0-27)
        y_one_hot = tf.keras.utils.to_categorical(y * 27, num_classes=28)
        
        return X, y_one_hot, self.scaler
    
    def prepare_counts_data(self, sequence_length=10):
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho d·ª± ƒëo√°n s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ng ch·ªØ s·ªë"""
        numbers = self.load_data()
        
        # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ng ch·ªØ s·ªë (0-9)
        digit_counts = []
        for num in numbers:
            digits = [int(d) for d in str(num).zfill(3)]
            counts = [digits.count(i) for i in range(10)]
            digit_counts.append(counts)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        digit_counts_normalized = self.scaler.fit_transform(digit_counts)
        
        # T·∫°o chu·ªói
        X, y = self.create_sequences(digit_counts_normalized, sequence_length)
        
        # Chuy·ªÉn ƒë·ªïi y v·ªÅ d·∫°ng one-hot encoding cho 10 s·ªë (0-9)
        # L·∫•y ch·ªØ s·ªë xu·∫•t hi·ªán nhi·ªÅu nh·∫•t l√†m target (tr∆∞·ªõc khi chu·∫©n h√≥a)
        y_digit = np.argmax(y, axis=1)
        y_one_hot = tf.keras.utils.to_categorical(y_digit, num_classes=10)
        
        print(f"Ph√¢n b·ªë ch·ªØ s·ªë trong d·ªØ li·ªáu counts:")
        unique, counts = np.unique(y_digit, return_counts=True)
        for digit, count in zip(unique, counts):
            print(f"  Ch·ªØ s·ªë {digit}: {count} l·∫ßn")
        
        return X, y_one_hot, self.scaler

class LotteryLSTMModel:
    """M√¥ h√¨nh LSTM cho d·ª± ƒëo√°n x·ªï s·ªë"""
    
    def __init__(self, input_shape, output_shape, model_type="raw_numbers"):
        self.input_shape = input_shape
        self.output_shape = output_shape
        self.model_type = model_type
        self.model = None
        self.history = None
        self.scaler = None  # Th√™m thu·ªôc t√≠nh scaler
        
    def build_model(self, lstm_units=128, dropout_rate=0.3):
        """X√¢y d·ª±ng m√¥ h√¨nh LSTM"""
        if self.model_type == "counts":
            # S·ª≠ d·ª•ng ki·∫øn tr√∫c ƒë·∫∑c bi·ªát cho counts v·ªõi regularization m·∫°nh h∆°n
            dropout_rate = 0.5  # TƒÉng dropout cho counts
            lstm_units = 64     # Gi·∫£m units ƒë·ªÉ tr√°nh overfitting
            
            model = keras.Sequential([
                # Input layer v·ªõi noise
                layers.GaussianNoise(0.1, input_shape=self.input_shape),
                
                # LSTM layers v·ªõi regularization m·∫°nh
                layers.LSTM(lstm_units, return_sequences=True, 
                          kernel_regularizer=keras.regularizers.l2(0.01),
                          recurrent_regularizer=keras.regularizers.l2(0.01)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.LSTM(lstm_units // 2, return_sequences=True,
                          kernel_regularizer=keras.regularizers.l2(0.01),
                          recurrent_regularizer=keras.regularizers.l2(0.01)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.LSTM(lstm_units // 4,
                          kernel_regularizer=keras.regularizers.l2(0.01),
                          recurrent_regularizer=keras.regularizers.l2(0.01)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                # Dense layers v·ªõi regularization
                layers.Dense(lstm_units // 2, activation='relu',
                           kernel_regularizer=keras.regularizers.l2(0.01)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.Dense(self.output_shape, activation='softmax')
            ])
        else:
            # Ki·∫øn tr√∫c c·∫£i ti·∫øn cho raw_numbers v√† sum
            dropout_rate = 0.4  # TƒÉng dropout
            lstm_units = 96     # Gi·∫£m units ƒë·ªÉ tr√°nh overfitting
            
            model = keras.Sequential([
                # Input layer v·ªõi noise nh·∫π
                layers.GaussianNoise(0.05, input_shape=self.input_shape),
                
                # LSTM layers v·ªõi regularization
                layers.LSTM(lstm_units, return_sequences=True, 
                          kernel_regularizer=keras.regularizers.l2(0.005),
                          recurrent_regularizer=keras.regularizers.l2(0.005)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.LSTM(lstm_units // 2, return_sequences=True,
                          kernel_regularizer=keras.regularizers.l2(0.005),
                          recurrent_regularizer=keras.regularizers.l2(0.005)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.LSTM(lstm_units // 4,
                          kernel_regularizer=keras.regularizers.l2(0.005),
                          recurrent_regularizer=keras.regularizers.l2(0.005)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                # Dense layers v·ªõi regularization
                layers.Dense(lstm_units // 2, activation='relu',
                           kernel_regularizer=keras.regularizers.l2(0.005)),
                layers.Dropout(dropout_rate),
                layers.BatchNormalization(),
                
                layers.Dense(self.output_shape, activation='softmax')
            ])
        
        # Compile model v·ªõi class weights n·∫øu l√† counts
        if self.model_type == "counts":
            # S·ª≠ d·ª•ng optimizer v√† learning rate ƒë·∫∑c bi·ªát cho counts
            optimizer = keras.optimizers.Adam(
                learning_rate=0.0005,  # Learning rate th·∫•p h∆°n
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-7
            )
            
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',
                metrics=['accuracy']  # Ch·ªâ s·ª≠ d·ª•ng accuracy c∆° b·∫£n
            )
        else:
            # S·ª≠ d·ª•ng optimizer v√† learning rate ƒë·∫∑c bi·ªát cho raw_numbers v√† sum
            optimizer = keras.optimizers.Adam(
                learning_rate=0.0008,  # Learning rate th·∫•p h∆°n
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-7
            )
            
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',
                metrics=['accuracy']
            )
        
        self.model = model
        return model
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):
        """Hu·∫•n luy·ªán m√¥ h√¨nh"""
        if self.model is None:
            self.build_model()
        
        # Callbacks
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True
        )
        
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=10,
            min_lr=1e-7
        )
        
        # Training v·ªõi class weights n·∫øu l√† counts
        if self.model_type == "counts":
            # T√≠nh class weights th·ª±c t·∫ø t·ª´ d·ªØ li·ªáu
            from sklearn.utils.class_weight import compute_class_weight # type: ignore
            y_train_labels = np.argmax(y_train, axis=1)
            class_weights = compute_class_weight(
                'balanced',
                classes=np.unique(y_train_labels),
                y=y_train_labels
            )
            class_weight_dict = dict(zip(range(10), class_weights))
            
            print(f"Class weights th·ª±c t·∫ø: {class_weight_dict}")
            
            # Data augmentation cho counts
            X_train_aug, y_train_aug = self._augment_counts_data(X_train, y_train)
            print(f"D·ªØ li·ªáu sau augmentation: {X_train_aug.shape}")
            
            # Gi·∫£m epochs cho counts ƒë·ªÉ tr√°nh overfitting
            counts_epochs = min(epochs, 50)
            print(f"S·ª≠ d·ª•ng {counts_epochs} epochs cho counts")
            
            self.history = self.model.fit(
                X_train_aug, y_train_aug,
                validation_data=(X_val, y_val),
                epochs=counts_epochs,
                batch_size=batch_size,
                callbacks=[early_stopping, reduce_lr],
                class_weight=class_weight_dict,
                verbose=1
            )
        else:
            # Data augmentation nh·∫π cho raw_numbers v√† sum
            X_train_aug, y_train_aug = self._augment_other_data(X_train, y_train)
            print(f"D·ªØ li·ªáu sau augmentation: {X_train_aug.shape}")
            
            # Gi·∫£m epochs ƒë·ªÉ tr√°nh overfitting
            other_epochs = min(epochs, 80)
            print(f"S·ª≠ d·ª•ng {other_epochs} epochs cho {self.model_type}")
            
            self.history = self.model.fit(
                X_train_aug, y_train_aug,
                validation_data=(X_val, y_val),
                epochs=other_epochs,
                batch_size=batch_size,
                callbacks=[early_stopping, reduce_lr],
                verbose=1
            )
        
        return self.history
    
    def _augment_counts_data(self, X, y):
        """Data augmentation cho d·ªØ li·ªáu counts"""
        if self.model_type != "counts":
            return X, y
        
        print("ƒêang th·ª±c hi·ªán data augmentation cho counts...")
        
        # T·∫°o d·ªØ li·ªáu m·ªõi b·∫±ng c√°ch th√™m noise nh·ªè
        X_aug = []
        y_aug = []
        
        # Th√™m d·ªØ li·ªáu g·ªëc
        X_aug.extend(X)
        y_aug.extend(y)
        
        # T·∫°o d·ªØ li·ªáu m·ªõi v·ªõi noise
        for i in range(len(X)):
            # Th√™m noise nh·ªè v√†o input
            noise = np.random.normal(0, 0.01, X[i].shape)
            X_noisy = X[i] + noise
            X_noisy = np.clip(X_noisy, 0, 1)  # Gi·ªØ trong kho·∫£ng [0, 1]
            
            X_aug.append(X_noisy)
            y_aug.append(y[i])
        
        # Th√™m d·ªØ li·ªáu v·ªõi rotation nh·ªè
        for i in range(len(X) // 2):  # Ch·ªâ l·∫•y m·ªôt n·ª≠a ƒë·ªÉ tr√°nh qu√° nhi·ªÅu
            # Xoay chu·ªói m·ªôt ch√∫t
            X_rotated = np.roll(X[i], shift=np.random.randint(-2, 3), axis=0)
            X_aug.append(X_rotated)
            y_aug.append(y[i])
        
        return np.array(X_aug), np.array(y_aug)
    
    def _augment_other_data(self, X, y):
        """Data augmentation nh·∫π cho raw_numbers v√† sum"""
        if self.model_type == "counts":
            return X, y
        
        print(f"ƒêang th·ª±c hi·ªán data augmentation nh·∫π cho {self.model_type}...")
        
        # T·∫°o d·ªØ li·ªáu m·ªõi b·∫±ng c√°ch th√™m noise nh·ªè
        X_aug = []
        y_aug = []
        
        # Th√™m d·ªØ li·ªáu g·ªëc
        X_aug.extend(X)
        y_aug.extend(y)
        
        # T·∫°o d·ªØ li·ªáu m·ªõi v·ªõi noise nh·ªè
        for i in range(len(X)):
            # Th√™m noise nh·ªè v√†o input
            noise = np.random.normal(0, 0.005, X[i].shape)  # Noise nh·ªè h∆°n counts
            X_noisy = X[i] + noise
            X_noisy = np.clip(X_noisy, 0, 1)  # Gi·ªØ trong kho·∫£ng [0, 1]
            
            X_aug.append(X_noisy)
            y_aug.append(y[i])
        
        # Th√™m d·ªØ li·ªáu v·ªõi rotation nh·ªè
        for i in range(len(X) // 3):  # Ch·ªâ l·∫•y 1/3 ƒë·ªÉ tr√°nh qu√° nhi·ªÅu
            # Xoay chu·ªói m·ªôt ch√∫t
            X_rotated = np.roll(X[i], shift=np.random.randint(-1, 2), axis=0)
            X_aug.append(X_rotated)
            y_aug.append(y[i])
        
        return np.array(X_aug), np.array(y_aug)
    
    def predict(self, X):
        """D·ª± ƒëo√°n"""
        if self.model is None:
            raise ValueError("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán")
        return self.model.predict(X)
    
    def save_model(self, filepath):
        """L∆∞u m√¥ h√¨nh"""
        if self.model is None:
            raise ValueError("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán")
        
        # S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng .keras thay v√¨ .h5 ƒë·ªÉ tr√°nh c·∫£nh b√°o
        if filepath.endswith('.h5'):
            filepath = filepath.replace('.h5', '.keras')
        
        self.model.save(filepath)
        print(f"ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i: {filepath}")
        
        # L∆∞u th√™m scaler ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y (n·∫øu c√≥)
        if self.scaler is not None:
            scaler_path = filepath.replace('.keras', '_scaler.npy')
            np.save(scaler_path, self.scaler, allow_pickle=True)
            print(f"ƒê√£ l∆∞u scaler t·∫°i: {scaler_path}")
        else:
            print("‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng c√≥ scaler ƒë·ªÉ l∆∞u")
    
    def plot_training_history(self):
        """V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán"""
        if self.history is None:
            print("Ch∆∞a c√≥ l·ªãch s·ª≠ hu·∫•n luy·ªán")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss
        ax1.plot(self.history.history['loss'], label='Training Loss')
        ax1.plot(self.history.history['val_loss'], label='Validation Loss')
        ax1.set_title('Model Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy
        ax2.plot(self.history.history['accuracy'], label='Training Accuracy')
        ax2.plot(self.history.history['val_accuracy'], label='Validation Accuracy')
        ax2.set_title('Model Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()

def cleanup_old_models(model_type, keep_latest=True):
    """X√≥a c√°c model c≈©, ch·ªâ gi·ªØ l·∫°i model m·ªõi nh·∫•t"""
    print(f"\nüßπ ƒêang d·ªçn d·∫πp model c≈© cho {model_type}...")
    
    try:
        # T√¨m t·∫•t c·∫£ file model v√† scaler
        model_pattern = f"lottery_model_{model_type}_*.keras"
        scaler_pattern = f"lottery_model_{model_type}_*_scaler.npy"
        
        model_files = glob.glob(model_pattern)
        scaler_files = glob.glob(scaler_pattern)
        
        print(f"üìÅ T√¨m th·∫•y {len(model_files)} file model v√† {len(scaler_files)} file scaler")
        
        if len(model_files) <= 1:
            print("‚úÖ Ch·ªâ c√≥ 1 model ho·∫∑c kh√¥ng c√≥ model, kh√¥ng c·∫ßn d·ªçn d·∫πp")
            return
        
        # S·∫Øp x·∫øp theo th·ªùi gian t·∫°o (m·ªõi nh·∫•t tr∆∞·ªõc)
        model_files.sort(key=os.path.getmtime, reverse=True)
        scaler_files.sort(key=os.path.getmtime, reverse=True)
        
        # X√≥a t·∫•t c·∫£ model c≈© (tr·ª´ model m·ªõi nh·∫•t n·∫øu keep_latest=True)
        files_to_delete = []
        
        if keep_latest:
            # Gi·ªØ l·∫°i model m·ªõi nh·∫•t
            files_to_delete.extend(model_files[1:])
            files_to_delete.extend(scaler_files[1:])
            print(f"üìå Gi·ªØ l·∫°i model m·ªõi nh·∫•t: {os.path.basename(model_files[0])}")
        else:
            # X√≥a t·∫•t c·∫£
            files_to_delete.extend(model_files)
            files_to_delete.extend(scaler_files)
        
        # X√≥a c√°c file
        deleted_count = 0
        for file_path in files_to_delete:
            try:
                os.remove(file_path)
                print(f"üóëÔ∏è  ƒê√£ x√≥a: {os.path.basename(file_path)}")
                deleted_count += 1
            except Exception as e:
                print(f"‚ùå Kh√¥ng th·ªÉ x√≥a {os.path.basename(file_path)}: {str(e)}")
        
        print(f"‚úÖ ƒê√£ x√≥a {deleted_count} file c≈©")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi d·ªçn d·∫πp model c≈©: {str(e)}")

class LotteryPredictor:
    """L·ªõp d·ª± ƒëo√°n x·ªï s·ªë"""
    
    def __init__(self, model, scaler, model_type):
        self.model = model
        self.scaler = scaler
        self.model_type = model_type
    
    def predict_next_numbers(self, recent_numbers, num_predictions=5):
        """D·ª± ƒëo√°n s·ªë ti·∫øp theo"""
        if self.model_type == "raw_numbers":
            return self._predict_raw_numbers(recent_numbers, num_predictions)
        elif self.model_type == "sum":
            return self._predict_sum(recent_numbers, num_predictions)
        elif self.model_type == "counts":
            return self._predict_counts(recent_numbers, num_predictions)
        else:
            raise ValueError("Lo·∫°i d·ª± ƒëo√°n kh√¥ng h·ª£p l·ªá")
    
    def _predict_raw_numbers(self, recent_numbers, num_predictions):
        """D·ª± ƒëo√°n s·ªë nguy√™n v·ªõi randomness"""
        # Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o
        numbers_normalized = self.scaler.transform(np.array(recent_numbers).reshape(-1, 1)).flatten()
        
        # D·ª± ƒëo√°n v·ªõi randomness
        predictions = []
        current_sequence = numbers_normalized[-10:].reshape(1, 10, 1)
        
        for _ in range(num_predictions):
            pred = self.model.predict(current_sequence, verbose=0)
            
            # S·ª≠ d·ª•ng temperature scaling ƒë·ªÉ tƒÉng randomness
            temperature = 1.5
            pred_scaled = pred[0] / temperature
            pred_probs = np.exp(pred_scaled) / np.sum(np.exp(pred_scaled))
            
            # L·∫•y top 5 predictions v√† ch·ªçn ng·∫´u nhi√™n
            top_5_indices = np.argsort(pred_probs)[-5:][::-1]
            top_5_probs = pred_probs[top_5_indices]
            
            # Ch·ªçn ng·∫´u nhi√™n t·ª´ top 5 v·ªõi x√°c su·∫•t t∆∞∆°ng ·ª©ng
            chosen_idx = np.random.choice(top_5_indices, p=top_5_probs/np.sum(top_5_probs))
            pred_normalized = chosen_idx / 999.0
            
            # Chuy·ªÉn v·ªÅ s·ªë nguy√™n
            pred_original = int(self.scaler.inverse_transform([[pred_normalized]])[0][0])
            predictions.append(pred_original)
            
            # C·∫≠p nh·∫≠t chu·ªói
            current_sequence = np.roll(current_sequence, -1, axis=1)
            current_sequence[0, -1, 0] = pred_normalized
        
        return predictions
    
    def _predict_sum(self, recent_numbers, num_predictions):
        """D·ª± ƒëo√°n t·ªïng c√°c ch·ªØ s·ªë v·ªõi randomness"""
        # T√≠nh t·ªïng c√°c ch·ªØ s·ªë
        sums = []
        for num in recent_numbers:
            digit_sum = sum(int(digit) for digit in str(num).zfill(3))
            sums.append(digit_sum)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        sums_normalized = self.scaler.transform(np.array(sums).reshape(-1, 1)).flatten()
        
        # D·ª± ƒëo√°n v·ªõi randomness
        predictions = []
        current_sequence = sums_normalized[-10:].reshape(1, 10, 1)
        
        for _ in range(num_predictions):
            pred = self.model.predict(current_sequence, verbose=0)
            
            # S·ª≠ d·ª•ng temperature scaling ƒë·ªÉ tƒÉng randomness
            temperature = 1.5
            pred_scaled = pred[0] / temperature
            pred_probs = np.exp(pred_scaled) / np.sum(np.exp(pred_scaled))
            
            # L·∫•y top 5 predictions v√† ch·ªçn ng·∫´u nhi√™n
            top_5_indices = np.argsort(pred_probs)[-5:][::-1]
            top_5_probs = pred_probs[top_5_indices]
            
            # Ch·ªçn ng·∫´u nhi√™n t·ª´ top 5 v·ªõi x√°c su·∫•t t∆∞∆°ng ·ª©ng
            chosen_idx = np.random.choice(top_5_indices, p=top_5_probs/np.sum(top_5_probs))
            pred_normalized = chosen_idx / 27.0
            
            # Chuy·ªÉn v·ªÅ t·ªïng g·ªëc
            pred_original = int(self.scaler.inverse_transform([[pred_normalized]])[0][0])
            predictions.append(pred_original)
            
            # C·∫≠p nh·∫≠t chu·ªói
            current_sequence = np.roll(current_sequence, -1, axis=1)
            current_sequence[0, -1, 0] = pred_normalized
        
        return predictions
    
    def _predict_counts(self, recent_numbers, num_predictions):
        """D·ª± ƒëo√°n ch·ªØ s·ªë xu·∫•t hi·ªán nhi·ªÅu nh·∫•t ti·∫øp theo"""
        # ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ng ch·ªØ s·ªë
        digit_counts = []
        for num in recent_numbers:
            digits = [int(d) for d in str(num).zfill(3)]
            counts = [digits.count(i) for i in range(10)]
            digit_counts.append(counts)
        
        # Chu·∫©n h√≥a d·ªØ li·ªáu
        digit_counts_normalized = self.scaler.transform(digit_counts)
        
        # D·ª± ƒëo√°n v·ªõi randomness
        predictions = []
        current_sequence = digit_counts_normalized[-10:].reshape(1, 10, 10)
        
        for i in range(num_predictions):
            pred = self.model.predict(current_sequence, verbose=0)
            
            # S·ª≠ d·ª•ng temperature scaling ƒë·ªÉ tƒÉng randomness
            temperature = 2.0
            pred_scaled = pred[0] / temperature
            pred_probs = np.exp(pred_scaled) / np.sum(np.exp(pred_scaled))
            
            # L·∫•y top 3 predictions v√† ch·ªçn ng·∫´u nhi√™n
            top_3_indices = np.argsort(pred_probs)[-3:][::-1]
            top_3_probs = pred_probs[top_3_indices]
            
            # Ch·ªçn ng·∫´u nhi√™n t·ª´ top 3 v·ªõi x√°c su·∫•t t∆∞∆°ng ·ª©ng
            chosen_idx = np.random.choice(top_3_indices, p=top_3_probs/np.sum(top_3_probs))
            predictions.append(chosen_idx)
            
            # C·∫≠p nh·∫≠t chu·ªói (s·ª≠ d·ª•ng one-hot encoding)
            one_hot = np.zeros(10)
            one_hot[chosen_idx] = 1
            current_sequence = np.roll(current_sequence, -1, axis=1)
            current_sequence[0, -1, :] = one_hot
        
        return predictions

def main():
    """H√†m ch√≠nh"""
    print("=== M√î H√åNH D·ª∞ ƒêO√ÅN X·ªî S·ªê S·ª¨ D·ª§NG RNN-LSTM ===\n")
    
    # C·∫•u h√¨nh
    DATA_FILE = "data-dacbiet.txt"
    SEQUENCE_LENGTH = 10
    EPOCHS = 100
    BATCH_SIZE = 32
    
    # Ki·ªÉm tra file d·ªØ li·ªáu
    if not os.path.exists(DATA_FILE):
        print(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {DATA_FILE}")
        return
    
    # X·ª≠ l√Ω d·ªØ li·ªáu
    processor = LotteryDataProcessor(DATA_FILE)
    
    # Danh s√°ch c√°c lo·∫°i d·ª± ƒëo√°n - ch·ªâ s·ª≠ d·ª•ng raw_numbers
    prediction_types = ["raw_numbers"]
    
    for pred_type in prediction_types:
        print(f"\n{'='*50}")
        print(f"ƒêANG X·ª¨ L√ù LO·∫†I D·ª∞ ƒêO√ÅN: {pred_type.upper()}")
        print(f"{'='*50}")
        
        try:
            # Chu·∫©n b·ªã d·ªØ li·ªáu
            if pred_type == "raw_numbers":
                X, y, scaler = processor.prepare_raw_numbers_data(SEQUENCE_LENGTH)
                output_shape = 1000
            elif pred_type == "sum":
                X, y, scaler = processor.prepare_sum_data(SEQUENCE_LENGTH)
                output_shape = 28
            elif pred_type == "counts":
                X, y, scaler = processor.prepare_counts_data(SEQUENCE_LENGTH)
                output_shape = 10
            
            print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu: X={X.shape}, y={y.shape}")
            
            # Chia d·ªØ li·ªáu
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # X√¢y d·ª±ng m√¥ h√¨nh
            if pred_type == "counts":
                input_features = X.shape[2]  # 10 features cho counts
            else:
                input_features = 1  # 1 feature cho raw_numbers v√† sum
            
            model_builder = LotteryLSTMModel(
                input_shape=(SEQUENCE_LENGTH, input_features),
                output_shape=output_shape,
                model_type=pred_type
            )
            
            # L∆∞u scaler v√†o model_builder
            model_builder.scaler = scaler
            
            # Hu·∫•n luy·ªán m√¥ h√¨nh
            print(f"\nB·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh {pred_type}...")
            history = model_builder.train(
                X_train, y_train, X_val, y_val,
                epochs=EPOCHS, batch_size=BATCH_SIZE
            )
            
            # ƒê√°nh gi√° m√¥ h√¨nh
            val_loss, val_accuracy = model_builder.model.evaluate(X_val, y_val, verbose=0)
            print(f"\nK·∫øt qu·∫£ hu·∫•n luy·ªán:")
            print(f"Validation Loss: {val_loss:.4f}")
            print(f"Validation Accuracy: {val_accuracy:.4f}")
            
            # L∆∞u m√¥ h√¨nh
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"lottery_model_{pred_type}_{timestamp}.keras"
            model_builder.save_model(model_filename)
            
            # D·ªçn d·∫πp model c≈© sau khi train th√†nh c√¥ng
            cleanup_old_models(pred_type, keep_latest=True)
            
            # V·∫Ω bi·ªÉu ƒë·ªì
            model_builder.plot_training_history()
            
            # D·ª± ƒëo√°n m·∫´u
            print(f"\nD·ª± ƒëo√°n m·∫´u cho {pred_type}:")
            predictor = LotteryPredictor(model_builder.model, scaler, pred_type)
            
            # L·∫•y 10 s·ªë g·∫ßn nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n
            recent_data = processor.load_data()[-10:]
            
            if pred_type == "raw_numbers":
                predictions = predictor.predict_next_numbers(recent_data, 255)
                print(f"10 s·ªë g·∫ßn nh·∫•t: {recent_data}")
                print(f"255 s·ªë d·ª± ƒëo√°n ti·∫øp theo (hi·ªÉn th·ªã 10 s·ªë ƒë·∫ßu): {predictions[:10]}...")
                print(f"T·ªïng c·ªông: {len(predictions)} s·ªë d·ª± ƒëo√°n")
            
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω {pred_type}: {str(e)}")
            continue
    
    print(f"\n{'='*50}")
    print("HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN T·∫§T C·∫¢ M√î H√åNH!")
    print(f"{'='*50}")

if __name__ == "__main__":
    main()
